runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
dim(train)
train = data.frame(test = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat, Time = tweets.df$created)
train = train[!is.na(train$lon),]
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
min(train$Time)
max(train$Time)
unique(train$lat)
length(unique(train$lat))
?searchTwitter
zillow_tweets <- searchTwitter("zillow",since='2016-09-01', until='2017-09-01'),resultType="popular", n=100)
zillow_tweets <- searchTwitter("zillow",since='2016-09-01', until='2017-09-01',resultType="popular", n=100)
tweets.df <- twListToDF(zillow_tweets)
tweets.df[1:10,]
zillow_tweets <- searchTwitter("zillow",since='2016-09-01', until='2017-09-01', n=100)
tweets.df <- twListToDF(zillow_tweets)
tweets.df[1:10,]
zillow_mentioners <- unique(tweets.df$screenName)
user_info <- twListToDF(lookupUsers(zillow_mentioners))
userTimeline(user_info$screenName[10], n=20)
geo_codes <- geocode(user_info$location)
dim(geo_codes)
geo_codes
user_info$location
head(tweets.df)
geo_codes <- geocode(tweets.df$location)
dim(geo_codes)
tweets.df$location
library(stringi)
stri_enc_mark(poli.dt$word)
stri_enc_mark(tweets.df$location)
stri_trans_general(tweets.df$location, "Latin-ASCII")
stri_trans_general(tweets.df$location, "ASCII")
geo_codes <- geocode(stri_trans_general(tweets.df$location, "Latin-ASCII"))
dim(geo_codes)
train = data.frame(test = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat, Time = tweets.df$created)
train = train[!is.na(train$lon),]
dim(train)
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
train
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
train
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
paste(min(train$Time))
paste(max(train$Time))
leafletProxy("map", data = train) %>%
clearShapes() %>%
addCircles(~lon, ~lat,
popup = ~text,
#color=~pal(Category),
radius= 10,
color = "#777777",
fillOpacity = 0.7)
leaflet(train) %>% addTiles() %>%
fitBounds(~min(lon), ~min(lat), ~max(lon), ~max(lat))
leafletProxy("map", data = train) %>%
clearShapes() %>%
addCircles(~lon, ~lat,
popup = ~text,
#color=~pal(Category),
radius= 10,
color = "#777777",
fillOpacity = 0.7)
})
leafletProxy("map", data = train) %>%
clearShapes() %>%
addCircles(~lon, ~lat,
popup = ~text,
#color=~pal(Category),
radius= 10,
color = "#777777",
fillOpacity = 0.7)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
summary(train)
library(stringi)
stri_enc_mark(tweets.df$location)
geo_codes <- geocode(stri_trans_general(tweets.df$location, "Latin-ASCII"))
dim(geo_codes)
train = data.frame(text = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat, Time = tweets.df$created)
train = train[!is.na(train$lon),]
dim(train)
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
train
dim(train)
train = data.frame(tweet_text = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat, Time = tweets.df$created)
train = train[!is.na(train$lon),]
dim(train)
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
train
dim(train)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
train$Time = ymd_hms(train$Time)
summary(train)
train %>%
filter_(interp(~ Time >= as.POSIXct(min(train$Time), tz = "UTC"), Dates = as.name("Dates")))
train %>%
filter_(interp(~ Time >= as.POSIXct(min(train$Time), tz = "UTC"), Dates = as.name("Dates"))) %>%
filter_(interp(~ Time <= as.POSIXct(max(train$Time), tz = "UTC"), Dates = as.name("Dates")))
leaflet(train) %>% addTiles() %>%
fitBounds(~min(lon), ~min(lat), ~max(lon), ~max(lat))
train = data.frame(tweet_text = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat, Time = tweets.df$created)
train = train[!is.na(train$lon),]
dim(train)
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
train
dim(train)
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
leaflet(data = train) %>% addTiles() %>%
addCircleMarkers(~lon, ~lat, popup = ~as.character(text))
leaflet(data = train) %>% addTiles() %>%
addCircleMarkers(~lon, ~lat, popup = ~text)
leaflet(data = train) %>% addTiles() %>%
addCircleMarkers(~lon, ~lat, popup = ~tweet_text)
leaflet(data = train) %>% addTiles() %>%
addCircleMarkers(~lon, ~lat, popup = ~tweet_text, fillOpacity = 0.5)
leaflet(data = train) %>% addTiles() %>%
addCircleMarkers(~lon, ~lat,
radius = 10,
popup = ~tweet_text, fillOpacity = 0.5)
leaflet(data = train) %>% addTiles() %>%
addCircleMarkers(~lon, ~lat,
radius = 1,
popup = ~tweet_text, fillOpacity = 0.5)
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
summary(train)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
summary(train)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
as.POSIXct(min(train$Time), tz = "UTC")
train$Time >= as.POSIXct(min(train$Time), tz = "UTC")
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
as.POSIXct(paste(min(train$Time)), tz = "UTC")
train %>%
filter_(interp(~ Time >= as.POSIXct(paste(min(train$Time)), tz = "UTC"), Dates = as.name("Dates")))
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
?dateRangeInput
input$dateRange[1]
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
summary(train)
class(train$Time)
train %>% filter(Time>=as.POSIXct("2016-06-01", tz = "UTC")) %>%
leaflet() %>% addTiles() %>%
addCircleMarkers(~lon, ~lat,
radius = 1,
popup = ~tweet_text, fillOpacity = 0.5)
train %>% filter(Time>=as.POSIXct("2016-12-01", tz = "UTC")) %>%
leaflet() %>% addTiles() %>%
addCircleMarkers(~lon, ~lat,
radius = 1,
popup = ~tweet_text, fillOpacity = 0.5)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
summary(train)
class(train$Time)
max(train$Time)
min(train$Time)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
min(train$Time)
max(train$Time)
zillow_tweets <- searchTwitter("zillow",since='2016-09-01', until='2017-09-01', n=1000)
tweets.df <- twListToDF(zillow_tweets)
tweets.df[1:10,]
zillow_mentioners <- unique(tweets.df$screenName)
user_info <- twListToDF(lookupUsers(zillow_mentioners))
userTimeline(user_info$screenName[10], n=20)
library(stringi)
stri_enc_mark(tweets.df$location)
geo_codes <- geocode(stri_trans_general(tweets.df$location, "Latin-ASCII"))
dim(geo_codes)
train = data.frame(tweet_text = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat, Time = tweets.df$created)
train = train[!is.na(train$lon),]
dim(train)
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
train
dim(train)
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
train %>% filter(Time>=as.POSIXct("2016-12-01", tz = "UTC")) %>%
leaflet() %>% addTiles() %>%
addCircleMarkers(~lon, ~lat,
radius = 0.1,
popup = ~tweet_text, fillOpacity = 0.5)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
#tweets evaluation function
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores <- laply(sentences, function(sentence, pos.words, neg.words){
sentence <- gsub('[[:punct:]]', "", sentence)
sentence <- gsub('[[:cntrl:]]', "", sentence)
sentence <- gsub('\\d+', "", sentence)
sentence <- tolower(sentence)
word.list <- str_split(sentence, '\\s+')
words <- unlist(word.list)
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress)
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
scores <- score.sentiment(train$tweet_text, pos.words, neg.words, .progress='text')
pos <- scan('data/positive-words.txt', what='character', comment.char=';') #folder with positive dictionary
neg <- scan('data/negative-words.txt', what='character', comment.char=';') #folder with negative dictionary
pos.words <- c(pos, 'upgrade')
neg.words <- c(neg, 'wtf', 'wait', 'waiting', 'epicfail')
scores <- score.sentiment(train$tweet_text, pos.words, neg.words, .progress='text')
summary(scores)
hist(scores)
hist(scores$score)
scores$sentiment = "neutral"
scores$sentiment[scores$score>0] = "positive"
scores$sentiment[scores$score<0] = "negative"
train = data.frame(train,score = scores$score,sentiment = scores$sentiment)
train
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
summary(train)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
train = data.frame(train,score = scores$score,Category = scores$sentiment)
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
summary(train)
train = data.frame(tweet_text = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat,
Time = tweets.df$created,
score = scores$score,Category = scores$sentiment)
train = train[!is.na(train$lon),]
dim(train)
train = data.frame(train)
train = data.frame(tweet_text = tweets.df$text,lon = geo_codes$lon, lat = geo_codes$lat,
Time = tweets.df$created)
train = train[!is.na(train$lon),]
dim(train)
train= data.frame(train,score = scores$score,Category = scores$sentiment)
train = data.frame(train)
train
summary(train)
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(train,"data/train.csv", row.names=FALSE)
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
summary(train)
class(train$Time)
max(train$Time)
min(train$Time)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
train = read.csv("data/train.csv")
class(train$Time)
train$Time = as.character(train$Time)
train$Time = ymd_hms(train$Time)
summary(train)
train %>% filter(Time>=as.POSIXct("2016-12-01", tz = "UTC")) %>%
filter(Category == unique(train$Category)) %>%
leaflet() %>% addTiles() %>%
addCircleMarkers(~lon, ~lat,
radius = 0.1,
popup = ~tweet_text, fillOpacity = 0.5)
unique(train$Category)
train %>% filter(Time>=as.POSIXct("2016-12-01", tz = "UTC")) %>%
filter(Category == c(unique(train$Category))) %>%
leaflet() %>% addTiles() %>%
addCircleMarkers(~lon, ~lat,
radius = 0.1,
popup = ~tweet_text, fillOpacity = 0.5)
unique(train$Category)
c(unique(train$Category))
c(as.character(unique(train$Category)))
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
unique(train$Category)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(wordcloud)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
myCorpus = Corpus(train$tweet_text)
myCorpus = train$tweet_text
myCorpus = tm_map(myCorpus, tolower)
myCorpus = as.character(train$tweet_text)
myCorpus = tm_map(myCorpus, tolower)
as.character(train$tweet_text)
train$tweet_text
myCorpus = readLines(train$tweet_text)
myCorpus =  Corpus(VectorSource(train$tweet_text))
myCorpus = tm_map(myCorpus, tolower)
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords, stopwords("english"))
myDTM = TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
set.seed(4363)
wordcloud(names(v), v, min.freq = 50)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "\\|")
myCorpus = tm_map(myCorpus, removeWords, stopwords("zillow"))
myCorpus = tm_map(myCorpus, removeWords, stemDocument)
myCorpus = tm_map(myCorpus, removeWords, "zillow")
myCorpus = tm_map(myCorpus, removeWords, stemDocument)
myCorpus = tm_map(myCorpus,  stemDocument)
myCorpus =  Corpus(VectorSource(train$tweet_text))
myCorpus = tm_map(myCorpus, tolower)
myCorpus = tm_map(myCorpus, removePunctuation)
myCorpus = tm_map(myCorpus, removeNumbers)
myCorpus = tm_map(myCorpus, removeWords, stopwords("english"))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "\\|")
myCorpus = tm_map(myCorpus, removeWords, "zillow")
myCorpus = tm_map(myCorpus,  stemDocument)
myDTM = TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
m = as.matrix(myDTM)
v = sort(rowSums(m), decreasing = TRUE)
head(v)
wordcloud(names(v), v,
min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
findFreqTerms(myDTM, lowfreq = 4)
findAssocs(myDTM, terms = "freedom", corlimit = 0.3)
head(v)
barplot(v[1:10,]$freq, las = 2, names.arg = v[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
d <- data.frame(word = names(v),freq=v)
head(d)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
col ="lightblue", main ="Most frequent words",
ylab = "Word frequencies")
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
myCorpus
m
v
match(d$word, pos.words)
d$word
d[!is.na(match(d$word, pos.words)),]
d = d[!is.na(match(d$word, pos.words)),]
set.seed(4363)
wordcloud(d$word, d$freq,
min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
d = d[!(is.na(match(d$word, pos.words)) & is.na(match(d$word, neg.words))),]
d <- data.frame(word = names(v),freq=v)
head(d)
d = d[!(is.na(match(d$word, pos.words)) & is.na(match(d$word, neg.words))),]
set.seed(4363)
wordcloud(d$word, d$freq,
min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
?wordcloud
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
d = d[!(is.na(match(d$word, pos.words))),]
set.seed(4363)
wordcloud(d$word, d$freq,
min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
head(d)
d=d[-1,]
head(d)
wordcloud(d$word, d$freq,
min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
setwd("S:/Data Science Think Tank/Twitter_visual")
write.csv(d,"data/clouddata.csv", row.names=FALSE)
wordcloud(d$word, d$freq,
min.freq = 1,
max.words=50, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
d = read.csv("data/clouddata.csv")
d
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
library(shiny)
runApp('S:/Data Science Think Tank/Twitter_visual',host="0.0.0.0",port=5050)
